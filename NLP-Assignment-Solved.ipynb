{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 1\n",
    "\n",
    "In this assignment we are meant to use the corpus (plain text) to generate N-gram models where n: 2-6. <br>\n",
    "The user should enter number of words (tokens) in the desired sentence, for example 10 words and should enter one word. <br>\n",
    "The output should have 10 words/tokens. We also going to evaluate 8 test samples by human expert (myself or my friend)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Code\n",
    "\n",
    "We start by importing the neccessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from random import choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the corpus path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'Corpus'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the user is allowed to set the desired n-gram.\n",
    "For testing purposes we sat it to 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n value: 3\n"
     ]
    }
   ],
   "source": [
    "n = int(input(\"Enter the value of n for the n-gram model: \"))\n",
    "print('n value:', n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we declare 4 dictionaries that are used to store n-grams, n-grams frequency, n-gram probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_counts = defaultdict(int)\n",
    "n_minus1_gram_counts = defaultdict(int)\n",
    "n_gram_frequency = defaultdict(int)\n",
    "n_gram_probs = defaultdict(dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_file function reads the passed file and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess_text as the name says it preprocess the text, toknize it and remove the non-arabic characters and the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]+', '', text) # Remove all non-Arabic characters\n",
    "    words = nltk.word_tokenize(text) # Tokenize the words\n",
    "    \n",
    "    filtered_words = [word for word in words if word not in string.punctuation and word != \"ØŒ\"] # Remove punctuation\n",
    "    \n",
    "    # arabic_stop_words = set(stopwords.words('arabic')) # Here we get the stop words of arabic Ù…Ø«Ù„: Ù‡Ø°Ù‡, Ù‡Ø°Ø§, Ùˆ\n",
    "    # filtered_words = [word for word in words if word.casefold() not in arabic_stop_words and word.isalpha()] # Here we remove these stop words and numbers\n",
    "    # filtered_words = [word for word in filtered_words if word not in string.punctuation] # Remove punctuation\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we loop through the files in the corpus and apply the prevois two functions then store the results in n_gram_counts\n",
    "(ğŸ’¡Note: The whole corpus are being used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir, _, files in os.walk(root_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(dir, file)\n",
    "            text = read_file(file_path)\n",
    "            words = preprocess_text(text)\n",
    "            for i in range(len(words) - n + 1):\n",
    "                n_gram = tuple(words[i:i+n])\n",
    "                n_gram_counts[n_gram] += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code maps a n-1 gram (i.e. the first n-1 words in a n-gram) to the total count of all n-grams that start with that same n-1 gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_gram, count in n_gram_counts.items():\n",
    "    n_minus1_gram = n_gram[:-1]\n",
    "    n_minus1_gram_counts[n_minus1_gram] += count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the top 10 most frequent n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent 3-grams:\n",
      "('ØµÙ„Ù‰', 'Ø§Ù„Ù„Ù‡', 'Ø¹Ù„ÙŠÙ‡'): 186980\n",
      "('Ø§Ù„Ù„Ù‡', 'Ø¹Ù„ÙŠÙ‡', 'ÙˆØ³Ù„Ù…'): 162840\n",
      "('Ø§Ù„Ù„Ù‡', 'ØµÙ„Ù‰', 'Ø§Ù„Ù„Ù‡'): 98126\n",
      "('Ø±Ø³ÙˆÙ„', 'Ø§Ù„Ù„Ù‡', 'ØµÙ„Ù‰'): 93524\n",
      "('Ø¹Ø¨Ø¯', 'Ø§Ù„Ù„Ù‡', 'Ø¨Ù†'): 73723\n",
      "('Ø§Ù„Ù†Ø¨ÙŠ', 'ØµÙ„Ù‰', 'Ø§Ù„Ù„Ù‡'): 68501\n",
      "('Ø¨Ù†', 'Ø¹Ø¨Ø¯', 'Ø§Ù„Ù„Ù‡'): 30801\n",
      "('Ø¹Ø¨Ø¯', 'Ø§Ù„Ø±Ø­Ù…Ù†', 'Ø¨Ù†'): 21843\n",
      "('Ø±Ø¶ÙŠ', 'Ø§Ù„Ù„Ù‡', 'Ø¹Ù†Ù‡'): 21212\n",
      "('Ø§Ù„Ù„Ù‡', 'Ø¹Ø²', 'ÙˆØ¬Ù„'): 20388\n"
     ]
    }
   ],
   "source": [
    "top_n = 10\n",
    "sorted_n_grams = sorted(n_gram_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "print(f\"Top {top_n} most frequent {n}-grams:\")\n",
    "for n_gram, count in sorted_n_grams:\n",
    "    print(f\"{n_gram}: {count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop calculate the probabilities of the n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_gram, count in n_gram_counts.items():\n",
    "    n_minus1_gram = n_gram[:-1]\n",
    "    prob = count / n_minus1_gram_counts[n_minus1_gram]\n",
    "    n_gram_probs[n_minus1_gram][n_gram[-1]] = prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print the probabilties, 10 samples of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 probabilities for ('Ø¨Ø³Ù…', 'Ø§Ù„Ù„Ù‡'):\n",
      "Ø§Ù„Ø±Ø­Ù…Ù†: 0.8827483196415236\n",
      "Ù…Ø¬Ø±Ø§Ù‡Ø§: 0.012447099825740602\n",
      "ÙˆØ§Ù„Ù„Ù‡: 0.006472491909385114\n",
      "Ù‚Ø§Ù„: 0.004729897933781429\n",
      "ÙˆØ¹Ù„Ù‰: 0.003236245954692557\n",
      "Ø§Ù„Ø°ÙŠ: 0.0027383619616629324\n",
      "Ø£Ø±Ù‚ÙŠÙƒ: 0.0027383619616629324\n",
      "Ø®ÙŠØ±: 0.0024894199651481204\n",
      "Ø«Ù…: 0.0022404779686333084\n",
      "ÙÙ„Ù…Ø§: 0.0019915359721184964\n",
      "\n",
      "Top 10 probabilities for ('Ø§Ù„Ù„Ù‡', 'Ø§Ù„Ø±Ø­Ù…Ù†'):\n",
      "Ø§Ù„Ø±Ø­ÙŠÙ…: 0.8947368421052632\n",
      "Ø§Ù„Ø±Ø­ÙŠÙ…ØŒ: 0.09844559585492228\n",
      "Ø§Ù„Ø±Ø­ÙŠÙ…ØŸ: 0.00218161985274066\n",
      "Ø§Ù„Ø±Ø­ÙŠÙ…Ø›: 0.000545404963185165\n",
      "Ø¹Ù„Ù‰: 0.0002727024815925825\n",
      "Ø¹Ø²: 0.0002727024815925825\n",
      "Øµ: 0.0002727024815925825\n",
      "Ø­ØªÙ‰: 0.0002727024815925825\n",
      "ÙØ¬Ø±Øª: 0.0002727024815925825\n",
      "ÙÙ„Ù…Ø§: 0.0002727024815925825\n",
      "\n",
      "Top 10 probabilities for ('Ø§Ù„Ø±Ø­Ù…Ù†', 'Ø§Ù„Ø±Ø­ÙŠÙ…'):\n",
      "Ù‚ÙˆÙ„Ù‡: 0.08283175695041559\n",
      "Ø³ÙˆØ±Ø©: 0.052163943823445115\n",
      "Ù…Ù†: 0.049011177987962166\n",
      "Ù‚Ø§Ù„: 0.039839495557466326\n",
      "Ø§Ù„Ø­Ù…Ø¯: 0.03210088850673545\n",
      "ÙƒØªØ§Ø¨: 0.02722843221553454\n",
      "Ù‡Ø°Ø§: 0.025508741759816565\n",
      "Ø±Ø¨: 0.020349670392662653\n",
      "Ù†Ø§: 0.01948982516480367\n",
      "Ø§Ù„Ù‚ÙˆÙ„: 0.01834336486099169\n",
      "\n",
      "Top 10 probabilities for ('Ø§Ù„Ø±Ø­ÙŠÙ…', 'Ø§Ù„Ø­Ù…Ø¯'):\n",
      "Ù„Ù„Ù‡: 0.9553571428571429\n",
      "Ù„Ù„Ù‡ØŒ: 0.026785714285714284\n",
      "ÙÙˆØµÙ„Øª: 0.008928571428571428\n",
      "Øµ: 0.008928571428571428\n",
      "\n",
      "Top 10 probabilities for ('Ø§Ù„Ø­Ù…Ø¯', 'Ù„Ù„Ù‡'):\n",
      "Ø§Ù„Ø°ÙŠ: 0.4433923425277892\n",
      "Ø±Ø¨: 0.1667352820090572\n",
      "Ø¹Ù„Ù‰: 0.03952243721696171\n",
      "Ø¨Ù„: 0.019761218608480857\n",
      "Ø­Ù…Ø¯Ø§: 0.011527377521613832\n",
      "Ø§Ù„Ø°Ù‰: 0.01029230135858378\n",
      "Ø§Ù„ÙˆÙ‡ÙˆØ¨: 0.009468917249897077\n",
      "ÙˆØ³Ù„Ø§Ù…: 0.008645533141210375\n",
      "ÙØ§Ø·Ø±: 0.008645533141210375\n",
      "Ø§Ù„ÙˆØ§Ø­Ø¯: 0.007410456978180321\n",
      "\n",
      "Top 10 probabilities for ('Ù„Ù„Ù‡', 'Ø±Ø¨'):\n",
      "Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†: 0.8060046189376443\n",
      "Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†ØŒ: 0.17090069284064666\n",
      "Ø§Ù„Ø¹Ù„Ù…ÙŠÙ†: 0.004618937644341801\n",
      "Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†Ø›: 0.003464203233256351\n",
      "Ø§Ù„Ø¹Ø²Ø©: 0.003464203233256351\n",
      "ÙƒÙ„: 0.0023094688221709007\n",
      "Ø§Ù„Ø¹Ø±Ø´: 0.0011547344110854503\n",
      "Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†ØŒØŒ: 0.0011547344110854503\n",
      "ÙÙŠ: 0.0011547344110854503\n",
      "Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†ØŸ: 0.0011547344110854503\n",
      "\n",
      "Top 10 probabilities for ('Ø±Ø¨', 'Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†'):\n",
      "Ù‚Ø§Ù„: 0.07025316455696203\n",
      "ÙˆØµÙ„Ù‰: 0.048734177215189876\n",
      "Ø­Ø¯Ø«Ù†Ø§: 0.030379746835443037\n",
      "Ø§Ù„ÙØ§ØªØ­Ø©: 0.022151898734177215\n",
      "ÙŠÙ‚ÙˆÙ„: 0.020253164556962026\n",
      "Ø«Ù…: 0.017721518987341773\n",
      "ÙˆØµÙ„ÙˆØ§ØªÙ‡: 0.01708860759493671\n",
      "Ù„Ø§: 0.016455696202531647\n",
      "Ø£Ù…: 0.016455696202531647\n",
      "Ø§Ù„Ø±Ø­Ù…Ù†: 0.012658227848101266\n",
      "\n",
      "Top 10 probabilities for ('Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†', 'Ø§Ù„Ø±Ø­Ù…Ù†'):\n",
      "Ø§Ù„Ø±Ø­ÙŠÙ…: 0.9523809523809523\n",
      "Ø§Ù„Ø±Ø­ÙŠÙ…ØŒ: 0.047619047619047616\n",
      "\n",
      "Top 10 probabilities for ('Ø§Ù„Ø±Ø­ÙŠÙ…', 'Ù…Ø§Ù„Ùƒ'):\n",
      "ÙŠÙˆÙ…: 0.9166666666666666\n",
      "Ø§Ù„Ø¥Ù…Ø§Ù…: 0.08333333333333333\n",
      "\n",
      "Top 10 probabilities for ('Ù…Ø§Ù„Ùƒ', 'ÙŠÙˆÙ…'):\n",
      "Ø§Ù„Ø¯ÙŠÙ†: 0.8176795580110497\n",
      "Ø§Ù„Ø¯ÙŠÙ†ØŒ: 0.06077348066298342\n",
      "Ø£Ø­Ø¯: 0.011049723756906077\n",
      "ÙŠØ¯Ø§Ù†: 0.011049723756906077\n",
      "Ø§Ù„Ø³Ø¨Øª: 0.011049723756906077\n",
      "Ø§Ù„Ù‚ÙŠØ§Ù…Ø©ØŒ: 0.0055248618784530384\n",
      "Ù‚Ø§Ù„: 0.0055248618784530384\n",
      "ØºØµØ¨Ù‡Ø§ØŒ: 0.0055248618784530384\n",
      "ÙŠØ³ØªØ­Ù‚Ù‡Ø§Ø›: 0.0055248618784530384\n",
      "ØºØµØ¨Ù‡ØŒ: 0.0055248618784530384\n",
      "\n",
      "Top 10 probabilities for ('ÙŠÙˆÙ…', 'Ø§Ù„Ø¯ÙŠÙ†'):\n",
      "Ù‚Ø§Ù„: 0.09821428571428571\n",
      "ÙŠÙˆÙ…: 0.05357142857142857\n",
      "Ø­Ø¯Ø«Ù†Ø§: 0.04017857142857143\n",
      "ÙŠÙ‚ÙˆÙ„: 0.04017857142857143\n",
      "Ø¥ÙŠØ§Ùƒ: 0.024553571428571428\n",
      "Ø§Ù„ÙØ§ØªØ­Ø©: 0.024553571428571428\n",
      "Ø£ÙŠ: 0.024553571428571428\n",
      "ÙŠØ¹Ù†ÙŠ: 0.017857142857142856\n",
      "Ù†Ø­Ù†: 0.015625\n",
      "Ø±Ø¨: 0.013392857142857142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_counter = 0\n",
    "for n_minus1_gram, prob_dict in n_gram_probs.items():\n",
    "    print(f\"Top 10 probabilities for {n_minus1_gram}:\")\n",
    "    sorted_probs = sorted(prob_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for next_word, prob in sorted_probs:\n",
    "        print(f\"{next_word}: {prob}\")\n",
    "    print()\n",
    "    temp_counter += 1\n",
    "    if temp_counter > 10:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method predicts the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_n_words(initial_words, num_words):\n",
    "    words = preprocess_text(initial_words)\n",
    "    num_words += n - 1\n",
    "    while len(words) < num_words:\n",
    "        n_minus1_gram = tuple(words[-(n-1):])\n",
    "        if n_minus1_gram not in n_gram_probs:\n",
    "            break\n",
    "        next_word = max(n_gram_probs[n_minus1_gram], key=n_gram_probs[n_minus1_gram].get)\n",
    "        words.append(next_word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This while loop prompt for the user the required number of initial words + how many words he want.\n",
    "An example is showing in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    initial_words = input(f\"Enter {n-1} initial words (separated by spaces) or 'q' to quit: \")\n",
    "    if initial_words == 'q':\n",
    "        break\n",
    "    num_words = int(input(\"Enter the number of words you want to generate: \"))\n",
    "    prediction = predict_next_n_words(initial_words, num_words)\n",
    "    print(f\"Generated text: {prediction}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated text: Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡ ÙÙ‚Ø§Ù„ Ù„Ù‡ ÙŠØ§ Ø£Ø¨Ø§ Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù†<br>\n",
    "Generated text: Ù…Ø­Ù…Ø¯ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù… Ù‚Ø§Ù„ Ù…Ù† ÙƒØ§Ù† ÙÙŠ Ø°Ù„Ùƒ Ù‚Ø§Ù„ Ø£Ù‡Ù„<br>\n",
    "Generated text: Ù…Ø­Ù…Ø¯ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù… Ù‚Ø§Ù„ Ù…Ù† ÙƒØ§Ù† ÙÙŠ Ø°Ù„Ùƒ Ù‚Ø§Ù„ Ø£Ù‡Ù„<br>\n",
    "Generated text: Ø§Ø¨Ùˆ Ø¨ÙƒØ± Ø¨Ù† Ø£Ø¨ÙŠ Ø·Ø§Ù„Ø¨ Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡ Ù‚Ø§Ù„ Ù‚Ø§Ù„ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡<br>\n",
    "Generated text: Ø¹Ù„ÙŠ Ø¨Ù† Ø£Ø¨ÙŠ Ø·Ø§Ù„Ø¨ Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡ Ù‚Ø§Ù„ Ù‚Ø§Ù„ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù… Ù‚Ø§Ù„ Ù…Ù† ÙƒØ§Ù† ÙÙŠ Ø°Ù„Ùƒ Ù‚Ø§Ù„ Ø£Ù‡Ù„<br>\n",
    "Generated text: ÙŠØ§ ØºÙ„Ø§Ù… Ø£Ø¹Ø·Ù‡ Ø£Ø±Ø¨Ø¹Ø© Ø¢Ù„Ø§Ù Ø¯Ø±Ù‡Ù… Ù‚Ø§Ù„ Ø£Ø¨Ùˆ Ø¬Ø¹ÙØ± ÙŠØ¹Ù†ÙŠ Ø¨Ø°Ù„Ùƒ Ø¬Ù„<br>\n",
    "Generated text: Ø§Ø¨Ùˆ Ø§Ø­Ù…Ø¯ Ø¨Ù† Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø£Ø¨ÙŠ Ø·Ø§Ù„Ø¨ Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡<br>\n",
    "Generated text: Ø§Ø¨Ùˆ Ø¬Ø¹ÙØ± Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø£Ø¨ÙŠ Ø·Ø§Ù„Ø¨ Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡<br>\n",
    "Generated text: Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø£Ø¨ÙŠ Ø·Ø§Ù„Ø¨ Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡ Ù‚Ø§Ù„ Ù‚Ø§Ù„ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡<br>\n",
    "Generated text: Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø¨Ù† Ø£Ø¨ÙŠ Ø·Ø§Ù„Ø¨ Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡ Ù‚Ø§Ù„ Ù‚Ø§Ù„ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡<br>\n",
    "Generated text: Ù…Ø§Ù„Ùƒ ÙŠÙˆÙ… Ø§Ù„Ø¯ÙŠÙ† Ù‚Ø§Ù„ Ø£Ø¨Ùˆ Ø¬Ø¹ÙØ± ÙŠØ¹Ù†ÙŠ Ø¨Ø°Ù„Ùƒ Ø¬Ù„ Ø«Ù†Ø§Ø¤Ù‡ Ø¨Ù‚ÙˆÙ„Ù‡ ÙˆÙ…Ù†<br>\n",
    "Generated text: Ø¥ÙŠØ§Ùƒ Ù†Ø¹Ø¨Ø¯ ÙˆØ¥ÙŠØ§Ùƒ Ù†Ø³ØªØ¹ÙŠÙ† Ø§Ù‡Ø¯Ù†Ø§ Ø§Ù„ØµØ±Ø§Ø· Ø§Ù„Ù…Ø³ØªÙ‚ÙŠÙ… ØµØ±Ø§Ø· Ø§Ù„Ø°ÙŠÙ† Ø£Ù†Ø¹Ù…Øª Ø¹Ù„ÙŠÙ‡Ù… ØºÙŠØ±"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-complete",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
